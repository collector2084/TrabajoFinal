{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Analizador de texto en Python: Bag of Words\n",
    "\n",
    "* * * \n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Objetivos de Aprendizaje \n",
    "    \n",
    "* Aprender c√≥mo convertir datos de texto en n√∫meros mediante el enfoque de Bag-of-Words (Bolsa de Palabras).\n",
    "* Comprender el algoritmo TF-IDF y c√≥mo complementa la representaci√≥n Bag-of-Words.\n",
    "* Implementar Bag-of-Words y TF-IDF usando el paquete `sklearn` y entender la configuraci√≥n de sus par√°metros.\n",
    "* Usar las representaciones num√©ricas de los datos de texto para realizar an√°lisis de sentimientos.\n",
    "</div>\n",
    "\n",
    "### √çconos Usados en Este Notebook\n",
    "üîî **Pregunta**: Una pregunta r√°pida para ayudarte a entender lo que est√° ocurriendo.<br>\n",
    "ü•ä **Desaf√≠o**: Ejercicio interactivo. ¬°Trabajaremos en ellos durante el taller!<br>\n",
    "üé¨ **Demostraci√≥n**: Mostrando algo m√°s avanzado ‚Äì ¬°para que sepas para qu√© puede usarse Python!<br> \n",
    "\n",
    "### Secciones\n",
    "1. [An√°lisis Exploratorio de Datos](#section1)\n",
    "2. [Preprocesamiento](#section2)\n",
    "3. [La Representaci√≥n Bag-of-Words](#section3)\n",
    "4. [Frecuencia de T√©rmino - Frecuencia Inversa de Documento (TF-IDF)](#section4)\n",
    "5. [Clasificaci√≥n de Sentimientos Usando la Representaci√≥n TF-IDF](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "En la parte anterior, aprendimos c√≥mo realizar el preprocesamiento de texto. Sin embargo, no fuimos m√°s all√° de los datos de texto en s√≠. Si estamos interesados en hacer alg√∫n an√°lisis computacional sobre los datos de texto, todav√≠a necesitamos enfoques para convertir ese texto en una **representaci√≥n num√©rica**.\n",
    "\n",
    "En la Parte 2 de nuestra serie de talleres, exploraremos una de las formas m√°s sencillas de generar una representaci√≥n num√©rica a partir del texto: el enfoque de **bag-of-words** (Bolsa de Palabras o BoW). Implementaremos la representaci√≥n BoW para transformar nuestros datos de tweets sobre aerol√≠neas, y luego construiremos un clasificador para explorar qu√© nos dice sobre el sentimiento de los tweets. En el coraz√≥n del enfoque bag-of-words se encuentra la suposici√≥n de que la frecuencia de ciertos tokens es informativa sobre la sem√°ntica y el sentimiento subyacente en el texto.\n",
    "\n",
    "Usaremos ampliamente el paquete `scikit-learn` para hacerlo, ya que proporciona un buen marco para construir la representaci√≥n num√©rica.\n",
    "\n",
    "¬°Primero instalemos `scikit-learn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Decomenta e intalla el paquete\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ed437f-9767-43b7-abc5-159aa4339a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomenta e install el paquete NLP packages introducido en la parte 1\n",
    "# %pip install NLTK\n",
    "# %pip install spaCy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3862ffd-918f-4184-8c90-8a39a8a2a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa otros paquetes necesarios\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea4a5-7c28-4557-acdd-afe8a97b7235",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# An√°lisis Exploratorio de Datos\n",
    "\n",
    "Antes de realizar cualquier tipo de preprocesamiento o modelado, siempre debemos llevar a cabo un an√°lisis exploratorio de datos para familiarizarnos con la informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer los datos\n",
    "tweets_path = 'data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "Como recordatorio, cada fila en este dataframe corresponde a un tweet. Las siguientes columnas son las que m√°s nos interesan. Hay otras columnas que contienen metadatos del tweet, como el autor, la fecha de creaci√≥n, la zona horaria del usuario, entre otros, que dejaremos de lado por ahora.\n",
    "\n",
    "- `text` (`str`): el texto del tweet.\n",
    "- `airline_sentiment` (`str`): el sentimiento del tweet, etiquetado como \"neutral\", \"positive\" o \"negative\".\n",
    "- `airline` (`str`): la aerol√≠nea sobre la que se tuitea.\n",
    "- `retweet count` (`int`): cu√°ntas veces fue retuiteado el tweet.\n",
    "\n",
    "Para prepararnos para la clasificaci√≥n de sentimientos, vamos a dividir el conjunto de datos y enfocarnos por ahora en los tweets \"positivos\" y \"negativos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1faaf90-8c01-4d25-9468-90c01823f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['airline_sentiment'] != 'neutral'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b039-53e7-4afe-a9e0-b3522c12b2d7",
   "metadata": {},
   "source": [
    "Vamos a echar un vistazo a algunos tweets primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "@VirginAmerica and it's a really big bad thing about it\n",
      "@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "@VirginAmerica yes, nearly every time I fly VX this ‚Äúear worm‚Äù won‚Äôt go away :)\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos 5 tweets\n",
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "¬°Ya podemos ver que algunos de estos tweets contienen sentimiento negativo! ¬øC√≥mo podemos darnos cuenta de esto?\n",
    "\n",
    "A continuaci√≥n, veamos la distribuci√≥n de etiquetas de sentimiento en este conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01955158-6954-447a-acb6-2989d02a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un grafico de la cantidad de tweet de los sentimientos\n",
    "sns.countplot(data=tweets,\n",
    "              x='airline_sentiment', \n",
    "              color='green',\n",
    "              order=['positive', 'negative']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab45abf-adf4-4f5e-ae09-75f6c4fd50d1",
   "metadata": {},
   "source": [
    "¬°Parece que la mayor√≠a de los tweets en este conjunto de datos expresan un sentimiento negativo!\n",
    "\n",
    "Veamos ahora qu√© tipo de tweets se retuitean m√°s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428ddde7-af73-4eb6-92c9-041a1791ca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    0.093375\n",
       "positive    0.069403\n",
       "Name: retweet_count, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos el recuento promedio de retweet de cada sentimiento\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f3bc-257c-48a8-86a0-fd0d7c3e8cb3",
   "metadata": {},
   "source": [
    "Los tweets negativos son claramente mas que los positivos\n",
    "Vamos a ver que aerolina recive mas tweets negativos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12aa9f2d-d655-494a-bb72-08ad973518f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>0.893760</td>\n",
       "      <td>0.106240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.146341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>0.842560</td>\n",
       "      <td>0.157440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>0.675399</td>\n",
       "      <td>0.324601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0.637091</td>\n",
       "      <td>0.362909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>0.543544</td>\n",
       "      <td>0.456456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative  positive\n",
       "airline                              \n",
       "US Airways         0.893760  0.106240\n",
       "American           0.853659  0.146341\n",
       "United             0.842560  0.157440\n",
       "Southwest          0.675399  0.324601\n",
       "Delta              0.637091  0.362909\n",
       "Virgin America     0.543544  0.456456"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos la porcion de tweets negativos por aerolinea\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042419e-9c41-40e7-8dbf-47bd1e2ad45a",
   "metadata": {},
   "source": [
    "¬°Parece que las personas est√°n m√°s insatisfechas con US Airways, seguida de American Airlines, ambas con m√°s del 85% de tweets negativos!\n",
    "\n",
    "Se podr√≠an hacer muchos descubrimientos interesantes si deseas explorar m√°s sobre los datos. Ahora volvamos a nuestra tarea de an√°lisis de sentimientos. Antes de eso, necesitamos preprocesar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Preprocesamiento\n",
    "\n",
    "Pasamos gran parte de la Parte 1 aprendiendo c√≥mo preprocesar datos. ¬°Vamos a aplicar lo que aprendimos! Al observar algunos de los tweets anteriores, podemos ver que, aunque est√°n en bastante buen estado, a√∫n podemos realizar un procesamiento adicional sobre ellos.\n",
    "\n",
    "En nuestra canalizaci√≥n (pipeline), omitiremos el proceso de tokenizaci√≥n ya que lo realizaremos en un paso posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 1: Aplicar una Canalizaci√≥n de Limpieza de Texto\n",
    "\n",
    "Escribe una funci√≥n llamada `preprocess` que realice los siguientes pasos sobre un texto de entrada:\n",
    "\n",
    "* Paso 1: Convertir todo el texto a min√∫sculas.\n",
    "* Paso 2: Reemplazar los siguientes patrones por marcadores de posici√≥n:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * D√≠gitos &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Menciones (handles) &rarr; ` USER `\n",
    "* Paso 3: Eliminar espacios en blanco extra.\n",
    "\n",
    "Aqu√≠ tienes algunas pistas para guiarte en este desaf√≠o:\n",
    "\n",
    "* Para el Paso 1, recuerda de la Parte 1 que existe un m√©todo de cadena llamado [`.lower()`](https://docs.python.org/3.11/library/stdtypes.html#str.lower) que puede usarse para convertir texto a min√∫sculas.\n",
    "* Hemos integrado el Paso 2 en una funci√≥n llamada `placeholder`. Ejecuta la celda de abajo para importarla en tu notebook, y podr√°s usarla como cualquier otra funci√≥n.\n",
    "* Para el Paso 3, te proporcionamos el patr√≥n regex para identificar caracteres de espacio en blanco, as√≠ como el reemplazo correcto para eliminar espacios extra.\n",
    "\n",
    "Ejecuta tu funci√≥n `preprocess` sobre `example_tweet` (tres celdas m√°s abajo) para verificar si funciona. Si lo hace, apl√≠cala a toda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21738b02-9ab9-4a61-b41f-ff75888aa747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import placeholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03569f0d-34ba-492d-aa1d-1dce9d34f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Crear una canalizaci√≥n de preprocesamiento que limpie los datos de los tweets.'''\n",
    "    \n",
    "    # Paso 1: Convertir a min√∫sculas\n",
    "    text = ...\n",
    "\n",
    "    # Paso 2: Reemplazar patrones con marcadores de posici√≥n\n",
    "    text = ...\n",
    "\n",
    "    # Paso 3: Eliminar caracteres de espacio en blanco extra\n",
    "    text = ...\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8990cefd-5d04-46ba-ada2-29978c28cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\n",
      "==================================================\n",
      "Ellipsis\n"
     ]
    }
   ],
   "source": [
    "example_tweet = 'lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo'\n",
    "\n",
    "# Imprimir el ejemplo de tweet\n",
    "print(example_tweet)\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Imprimir el preproceso del tweet\n",
    "print(preprocess(example_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Ellipsis\n",
       "1    Ellipsis\n",
       "2    Ellipsis\n",
       "3    Ellipsis\n",
       "4    Ellipsis\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicar la funci√≥n a la columna de texto y asignar los tweets preprocesados a una nueva columna\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "¬°Felicidades! El preprocesamiento est√° completo. ¬°Vamos a sumergirnos en el bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# La Representaci√≥n Bag-of-Words\n",
    "\n",
    "La idea del enfoque bag-of-words (BoW), como sugiere el nombre, es bastante intuitiva: tomamos un documento y lo \"lanzamos\" a una bolsa. El acto de \"arrojar\" el documento a una bolsa ignora la posici√≥n relativa entre las palabras, por lo que lo que queda \"en la bolsa\" es esencialmente un \"conjunto desordenado de palabras\" [(Jurafsky & Martin, 2024)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). A cambio, obtenemos una lista de palabras √∫nicas y la frecuencia de cada una de ellas.\n",
    "\n",
    "Por ejemplo, como se muestra en la siguiente ilustraci√≥n, la palabra \"coffee\" aparece dos veces.\n",
    "\n",
    "<img src='../images/bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "Con una representaci√≥n bag-of-words, hacemos un uso intensivo de la frecuencia de las palabras, pero no tanto del orden en que aparecen.\n",
    "\n",
    "En el contexto del an√°lisis de sentimientos, el sentimiento de un tweet se transmite con m√°s fuerza a trav√©s de palabras espec√≠ficas. Por ejemplo, si un tweet contiene la palabra \"happy\" (feliz), probablemente exprese un sentimiento positivo, aunque no siempre (por ejemplo, \"not happy\" denota el sentimiento opuesto). Cuando estas palabras aparecen con m√°s frecuencia, probablemente transmitan con m√°s fuerza el sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Matriz Documento-T√©rmino\n",
    "\n",
    "Ahora implementemos la idea de bag-of-words. Antes de profundizar m√°s, retrocedamos un momento. En la pr√°ctica, el an√°lisis de texto a menudo implica manejar muchos documentos; a partir de ahora, usaremos el t√©rmino **documento** para representar un fragmento de texto sobre el cual realizamos an√°lisis. Puede ser una frase, una oraci√≥n, un tweet u otro tipo de texto‚Äîmientras pueda representarse como una cadena de texto, la longitud no importa realmente.\n",
    "\n",
    "Imagina que tenemos cuatro documentos (es decir, las cuatro frases mostradas arriba), y los lanzamos todos a la bolsa. En lugar de obtener una lista de frecuencias de palabras, obtendremos como resultado una matriz documento-t√©rmino (DTM, por sus siglas en ingl√©s). En una DTM, la lista de palabras es el **vocabulario** (V), que contiene todas las palabras √∫nicas que aparecen en los documentos. Para cada **documento** (D), contamos cu√°ntas veces aparece cada palabra del vocabulario, y luego colocamos ese n√∫mero en la matriz. En otras palabras, la DTM que construiremos es una matriz de tama√±o $D \\times V$, donde cada fila corresponde a un documento, y cada columna corresponde a un token (o \"t√©rmino\").\n",
    "\n",
    "Los tokens √∫nicos en este conjunto de documentos, ordenados alfab√©ticamente, forman las columnas. Para cada documento, marcamos la aparici√≥n de cada palabra presente. La representaci√≥n num√©rica de cada documento es una fila de la matriz. Por ejemplo, el primer documento, \"the coffee roaster\", tiene la representaci√≥n num√©rica $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Nota que la columna de √≠ndice a la izquierda muestra estos documentos como texto, pero normalmente simplemente asignar√≠amos un √≠ndice a cada uno.\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Para crear una DTM, utilizaremos `CountVectorizer` del paquete `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989781d-6b40-417a-be70-eeba05cd8a50",
   "metadata": {},
   "source": [
    "La siguiente ilustraci√≥n muestra el flujo de trabajo en tres pasos para crear una DTM con `CountVectorizer`.\n",
    "\n",
    "<img src='../images/CountVectorizer1.png' alt=\"CountVectorizer\" width=\"500\">\n",
    "\n",
    "Vamos a recorrer estos pasos utilizando el ejemplo simple mostrado arriba.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174034-46b9-43e2-a511-5972d378cb00",
   "metadata": {},
   "source": [
    "### Un ejemplo de juego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4da2bd3d-0460-4b5f-9b9e-02940db0d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents\n",
    "test = ['the coffee roaster',\n",
    "        'light roast',\n",
    "        'iced americano',\n",
    "        'coffee time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c1d3-fcee-4e20-b9a7-17306ebd5fc2",
   "metadata": {},
   "source": [
    "El primer paso es inicializar un objeto `CountVectorizer`. Dentro de los par√©ntesis, podemos especificar configuraciones de par√°metros si lo deseamos. Echemos un vistazo a la [documentaci√≥n](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) para ver qu√© opciones est√°n disponibles.\n",
    "\n",
    "Por ahora, podemos dejarlo en blanco para usar la configuraci√≥n predeterminada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9de3fe6a-9abf-4e11-aad1-e54c891567bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d0d-0bfc-4fb9-8e5f-e91e39797fb5",
   "metadata": {},
   "source": [
    "El segundo paso es aplicar el m√©todo `fit` al objeto `CountVectorizer` con los datos, lo que significa crear un vocabulario de tokens a partir del conjunto de documentos. En tercer lugar, usamos `transform` sobre nuestros datos usando el objeto `CountVectorizer` ya \"ajustado\", lo que implica tomar cada documento y contar las ocurrencias de los tokens seg√∫n el vocabulario establecido durante el paso de \"ajuste\" (`fit`).\n",
    "\n",
    "Puede sonar un poco complejo, pero los pasos 2 y 3 se pueden realizar de una sola vez usando la funci√≥n `fit_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da1bbad4-bb1a-4b92-9096-6e17558b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform to create a DTM\n",
    "test_count = vectorizer.fit_transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3b65-4e98-48bf-87d2-399457f4939c",
   "metadata": {},
   "source": [
    "El resultado de `fit_transform` deber√≠a ser la DTM (Matriz Documento-T√©rmino).\n",
    "\n",
    "¬°Vamos a echarle un vistazo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb044001-8eb2-4489-b025-2d8e2d4bfee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9817b09-a806-42c4-9436-822cc27a38b9",
   "metadata": {},
   "source": [
    "Aparentemente, lo que obtuvimos es una \"matriz dispersa\" (sparse matrix), es decir, una matriz que contiene muchos ceros. Esto tiene sentido. Para cada documento, hay palabras que no aparecen en absoluto, y estas se contabilizan como ceros en la DTM. Esta matriz dispersa se almacena en un formato llamado \"Compressed Sparse Row\" (CSR), un formato optimizado para ahorrar memoria al manejar matrices dispersas.\n",
    "\n",
    "Vamos a convertirla en una matriz densa, donde probablemente esos ceros est√©n representados, como en un array de NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb03a238-87d8-40c9-b20e-66e7c9b6576b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b58a63-d7f6-4b9f-aadf-4d4fc7341336",
   "metadata": {},
   "source": [
    "¬°As√≠ que esta es nuestra DTM! La matriz es la misma que mostramos anteriormente. Para que sea m√°s f√°cil de leer, vamos a convertirla en un dataframe. Los nombres de las columnas deben ser los tokens del vocabulario, los cuales podemos obtener con la funci√≥n `get_feature_names_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714de5d3-e37d-4a19-9ade-3c6629e38d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['americano', 'coffee', 'iced', 'light', 'roast', 'roaster', 'the',\n",
       "       'time'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a7729a2-ca2e-4de7-8795-74dfedb7a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(),\n",
    "                        columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781da407-f394-40f2-9d45-1fac39f02047",
   "metadata": {},
   "source": [
    "¬°Aqu√≠ est√°! La DTM de nuestros datos de ejemplo ahora es un dataframe. El √≠ndice de `test_dtm` corresponde a la posici√≥n de cada documento en la lista `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e41dd243-cd2e-43c3-80f8-5eaab6e64210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano  coffee  iced  light  roast  roaster  the  time\n",
       "0          0       1     0      0      0        1    1     0\n",
       "1          0       0     0      1      1        0    0     0\n",
       "2          1       0     1      0      0        0    0     0\n",
       "3          0       1     0      0      0        0    0     1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a03b4-94fa-4fe7-8f5d-7280e31b9bc4",
   "metadata": {},
   "source": [
    "Esperamos que este ejemplo sencillo haya servido como una gu√≠a clara para crear una DTM.\n",
    "\n",
    "¬°Ahora es momento de trabajar con nuestros datos de tweets!\n",
    "\n",
    "### DTM para Tweets\n",
    "\n",
    "Comenzaremos inicializando un objeto `CountVectorizer`. En la siguiente celda, hemos incluido algunos par√°metros que la gente suele ajustar. Estos par√°metros est√°n actualmente configurados con sus valores predeterminados.\n",
    "\n",
    "Cuando construimos una DTM, el comportamiento por defecto es convertir el texto de entrada a min√∫sculas. Si no se proporciona nada en `stop_words`, el valor por defecto es conservarlas. Los siguientes tres par√°metros se utilizan para controlar el tama√±o del vocabulario, a los cuales volveremos en un momento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "0    Ellipsis\n",
      "1    Ellipsis\n",
      "2    Ellipsis\n",
      "3    Ellipsis\n",
      "4    Ellipsis\n",
      "Name: text_processed, dtype: object\n",
      "text_processed\n",
      "<class 'ellipsis'>    11541\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words=None,\n",
    "                             min_df=1,\n",
    "                             max_df=1.0, \n",
    "                             max_features=None)\n",
    "print(tweets['text_processed'].dtype)  # Tipo de datos de la columna\n",
    "print(tweets['text_processed'].head())  # Muestra las primeras filas\n",
    "print(tweets['text_processed'].apply(type).value_counts())  # Tipos de valores en la columna\n",
    "\n",
    "# prueba\n",
    "from utils import placeholder # type: ignore\n",
    " \n",
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    " \n",
    "def preprocess(text):\n",
    "    # Step 1: Minusculas\n",
    "    text = text.lower()\n",
    " \n",
    "    # Step 2: Renplazar patrones con marcadores de posici√≥n\n",
    "    text = placeholder(text)\n",
    " \n",
    "    # Step 3: Remover los caracteres con espacios extras\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    " \n",
    "    return text\n",
    " \n",
    "tweets['text_processed'] = tweets['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11541x8751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 191139 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustar y transformar para crear la DTM\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'].fillna(''))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No ejecutes esto si tienes memoria limitada ‚Äî esto incluye DataHub y Binder.\n",
    "np.array(counts.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer tokens\n",
    "tokens = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43620587-3795-4434-8f1f-145c81b93706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 8751)\n"
     ]
    }
   ],
   "source": [
    "# Crear DTM\n",
    "first_dtm = pd.DataFrame(data=counts.todense(),\n",
    "                         index=tweets.index,\n",
    "                         columns=tokens)\n",
    "\n",
    "# Imprimir la forma (shape) de la DTM\n",
    "print(first_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd257d5-4244-436c-afe7-5688232caf8f",
   "metadata": {},
   "source": [
    "Si dejamos `CountVectorizer` con la configuraci√≥n predeterminada, el tama√±o del vocabulario en los datos de tweets es de 8751."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb3604ec-d909-4238-9a3f-67e7d4ae2ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_exact_</th>\n",
       "      <th>_wtvd</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>...</th>\n",
       "      <th>zero</th>\n",
       "      <th>zig</th>\n",
       "      <th>zip</th>\n",
       "      <th>zippers</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zukes</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 8751 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _exact_  _wtvd  aa  aaaand  aadv  aadvantage  aal  aaron  ab  aback  ...  \\\n",
       "0        0      0   0       0     0           0    0      0   0      0  ...   \n",
       "1        0      0   0       0     0           0    0      0   0      0  ...   \n",
       "2        0      0   0       0     0           0    0      0   0      0  ...   \n",
       "3        0      0   0       0     0           0    0      0   0      0  ...   \n",
       "4        0      0   0       0     0           0    0      0   0      0  ...   \n",
       "\n",
       "   zero  zig  zip  zippers  zone  zones  zoom  zukes  zurich  zz  \n",
       "0     0    0    0        0     0      0     0      0       0   0  \n",
       "1     0    0    0        0     0      0     0      0       0   0  \n",
       "2     0    0    0        0     0      0     0      0       0   0  \n",
       "3     0    0    0        0     0      0     0      0       0   0  \n",
       "4     0    0    0        0     0      0     0      0       0   0  \n",
       "\n",
       "[5 rows x 8751 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d34e2-52f8-4419-b4c7-ed20dbd5df89",
   "metadata": {},
   "source": [
    "La mayor√≠a de los tokens tienen cero apariciones al menos en los primeros cinco tweets.\n",
    "\n",
    "¬°Echemos un vistazo m√°s de cerca a la DTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      12882\n",
       "to         6987\n",
       "digit      6927\n",
       "the        5088\n",
       "you        3635\n",
       "for        3386\n",
       "flight     3320\n",
       "and        3276\n",
       "on         3142\n",
       "my         2751\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens mas frecuentes\n",
    "first_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26c7f1c9-dd66-49f2-b337-01253da551d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_exact_                     1\n",
       "mightmismybrosgraduation    1\n",
       "midterm                     1\n",
       "midnite                     1\n",
       "midland                     1\n",
       "michelle                    1\n",
       "michele                     1\n",
       "michael                     1\n",
       "mhtt                        1\n",
       "mgmt                        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens menos frecuentes\n",
    "first_dtm.sum().sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230f79-e752-4e32-93db-4f013287f8e2",
   "metadata": {},
   "source": [
    "No es sorprendente ver que \"user\" y \"digit\" est√©n entre los tokens m√°s frecuentes, ya que reemplazamos cada uno de los elementos idiosincr√°ticos con estos marcadores de posici√≥n. El resto de los tokens m√°s frecuentes son en su mayor√≠a palabras vac√≠as (stop words).\n",
    "\n",
    "Quiz√°s un patr√≥n m√°s interesante sea buscar qu√© token aparece m√°s en un tweet determinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efb8f4d8-4c88-4155-a6c5-c72a5b4e8bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>lt</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>worst</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10572</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8148</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10742</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10923</th>\n",
       "      <td>the</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7750</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  number\n",
       "3127      lt       6\n",
       "918    worst       6\n",
       "10572     to       5\n",
       "8148     the       5\n",
       "10742     to       5\n",
       "152       to       5\n",
       "5005      to       5\n",
       "10923    the       5\n",
       "7750      to       5\n",
       "355       to       5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = pd.DataFrame()\n",
    "\n",
    "# Obtener el √≠ndice del tweet donde un token aparece con mayor frecuencia\n",
    "counts['token'] = first_dtm.idxmax(axis=1)\n",
    "\n",
    "# Obtener el n√∫mero de ocurrencias\n",
    "counts['number'] = first_dtm.max(axis=1)\n",
    "\n",
    "# Filtrar los marcadores de posici√≥n\n",
    "counts[(counts['token'] != 'digit')\n",
    "       & (counts['token'] != 'hashtag')\n",
    "       & (counts['token'] != 'user')].sort_values('number', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdac4ef-6b9d-4aad-9b24-c70f6c2eb8f0",
   "metadata": {},
   "source": [
    "Parece que, entre todos los tweets, como m√°ximo un token aparece seis veces, y es o bien la palabra \"It\" o la palabra \"worst\".\n",
    "\n",
    "Volvamos a nuestro dataframe de tweets y localicemos el tweet n√∫mero 918."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e7cacd8-1fb3-4f0d-a744-4ee0994a089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united is the worst. Worst reservation policies. Worst costumer service. Worst worst worst. Congrats, @Delta you're not that bad!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve 918th tweet: \"worst\"\n",
    "tweets.iloc[918]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Personalizar el `CountVectorizer`\n",
    "\n",
    "Hasta ahora hemos utilizado siempre la configuraci√≥n predeterminada para crear nuestras DTM, pero en muchos casos podr√≠amos querer personalizar el objeto `CountVectorizer`. El prop√≥sito de hacerlo es filtrar a√∫n m√°s los tokens innecesarios. En el siguiente ejemplo, ajustamos los siguientes par√°metros:\n",
    "\n",
    "- `stop_words = 'english'`: ignora las palabras vac√≠as en ingl√©s  \n",
    "- `min_df = 2`: ignora las palabras que no aparecen al menos dos veces  \n",
    "- `max_df = 0.95`: ignora las palabras que aparecen en m√°s del 95% de los documentos  \n",
    "\n",
    "üîî **Pregunta**: ¬°Pausamos un momento para discutir si tiene sentido establecer estos par√°metros! ¬øQu√© opinas?\n",
    "\n",
    "A menudo, no nos interesan las palabras cuya frecuencia es demasiado baja o demasiado alta, por eso usamos `min_df` y `max_df` para filtrarlas. Alternativamente, podemos definir el tama√±o de nuestro vocabulario como $N$ usando `max_features`. En otras palabras, le indicamos a `CountVectorizer` que solo considere los $N$ tokens m√°s frecuentes al construir la DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the parameter setting\n",
    "vectorizer = CountVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b53e5ecf-7be3-4915-9d11-fd3edb913400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create the second DTM\n",
    "second_dtm = pd.DataFrame(data=counts.todense(),\n",
    "                          index=tweets.index,\n",
    "                          columns=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e66bc-2eaa-4642-8848-74459948084b",
   "metadata": {},
   "source": [
    "Nuestra segunda DTM tiene un vocabulario sustancialmente m√°s peque√±o en comparaci√≥n con la primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "570fb598-fa81-4111-9e36-7172d8034713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11541, 8751)\n",
      "(11541, 4471)\n"
     ]
    }
   ],
   "source": [
    "print(first_dtm.shape)\n",
    "print(second_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8deabb2-20eb-4047-b592-48cb1564fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aadv</th>\n",
       "      <th>aadvantage</th>\n",
       "      <th>aal</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abq</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 4471 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aadv  aadvantage  aal  abandoned  abc  ability  able  aboard  abq  ...  \\\n",
       "0   0     0           0    0          0    0        0     0       0    0  ...   \n",
       "1   0     0           0    0          0    0        0     0       0    0  ...   \n",
       "2   0     0           0    0          0    0        0     0       0    0  ...   \n",
       "3   0     0           0    0          0    0        0     0       0    0  ...   \n",
       "4   0     0           0    0          0    0        0     0       0    0  ...   \n",
       "\n",
       "   yummy  yup  yvonne  yvr  yyj  yyz  zero  zone  zoom  zurich  \n",
       "0      0    0       0    0    0    0     0     0     0       0  \n",
       "1      0    0       0    0    0    0     0     0     0       0  \n",
       "2      0    0       0    0    0    0     0     0     0       0  \n",
       "3      0    0       0    0    0    0     0     0     0       0  \n",
       "4      0    0       0    0    0    0     0     0     0       0  \n",
       "\n",
       "[5 rows x 4471 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe2c3-ec90-4027-8c7f-417327a33a27",
   "metadata": {},
   "source": [
    "La lista de tokens m√°s frecuentes ahora incluye palabras que tienen m√°s sentido para nosotros, como \"cancelled\" y \"service\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffa7bf4e-640b-49bc-b64b-721140f67f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "digit        6927\n",
       "flight       3320\n",
       "hashtag      2633\n",
       "cancelled     956\n",
       "thanks        921\n",
       "service       910\n",
       "just          801\n",
       "customer      726\n",
       "time          695\n",
       "help          687\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## ü•ä Desaf√≠o 2: Lematizar el Texto de Entrada\n",
    "\n",
    "Recuerda que en la Parte 1 presentamos c√≥mo usar `spaCy` para realizar lematizaci√≥n, es decir, \"recuperar\" la forma base de una palabra. Este proceso ayuda a reducir el tama√±o del vocabulario manteniendo al m√≠nimo las variaciones de una palabra‚Äîun vocabulario m√°s peque√±o puede mejorar el rendimiento del modelo en la clasificaci√≥n de sentimientos.\n",
    "\n",
    "Ahora implementaremos la lematizaci√≥n en nuestros datos de tweets y usaremos el texto lematizado para crear una tercera DTM.\n",
    "\n",
    "Completa la funci√≥n `lemmatize_text`. Esta funci√≥n recibe un texto como entrada y devuelve los lemas de todos los tokens.\n",
    "\n",
    "Aqu√≠ tienes algunas pistas para ayudarte en este desaf√≠o:\n",
    "\n",
    "- Paso 1: inicializa una lista para almacenar los lemas  \n",
    "- Paso 2: aplica la canalizaci√≥n `nlp` al texto de entrada  \n",
    "- Paso 3: itera sobre los tokens en el texto procesado y recupera el lema de cada token  \n",
    "    - PISTA: la lematizaci√≥n es una de las anotaciones ling√º√≠sticas que la canalizaci√≥n `nlp` realiza autom√°ticamente. Podemos usar `token.lemma_` para acceder a esa anotaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a908f0-f0c6-4ba0-92be-447004f298f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
